{"docstore/metadata": {"a557ab59-0c2d-4d6a-8818-0a01df6822b4": {"doc_hash": "442f86ed99e0efa803562a8e17c21199f10f73e0a350f9a1ef410911bfdcc974"}, "e0588441-01c4-4f03-b12b-8ce079eda9bb": {"doc_hash": "c798edd22226056308adf41ee312f3e1e4f2348befabb4391ba8c9a897b69056", "ref_doc_id": "a557ab59-0c2d-4d6a-8818-0a01df6822b4"}}, "docstore/ref_doc_info": {"a557ab59-0c2d-4d6a-8818-0a01df6822b4": {"node_ids": ["e0588441-01c4-4f03-b12b-8ce079eda9bb"], "metadata": {"page_label": "1", "file_name": "paper.pdf", "file_path": "/app/rags/medical/data/paper.pdf", "file_type": "application/pdf", "file_size": 2468446, "creation_date": "2025-11-02", "last_modified_date": "2025-11-02"}}}, "docstore/data": {"e0588441-01c4-4f03-b12b-8ce079eda9bb": {"__data__": {"id_": "e0588441-01c4-4f03-b12b-8ce079eda9bb", "embedding": null, "metadata": {"page_label": "1", "file_name": "paper.pdf", "file_path": "/app/rags/medical/data/paper.pdf", "file_type": "application/pdf", "file_size": 2468446, "creation_date": "2025-11-02", "last_modified_date": "2025-11-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a557ab59-0c2d-4d6a-8818-0a01df6822b4", "node_type": "4", "metadata": {"page_label": "1", "file_name": "paper.pdf", "file_path": "/app/rags/medical/data/paper.pdf", "file_type": "application/pdf", "file_size": 2468446, "creation_date": "2025-11-02", "last_modified_date": "2025-11-02"}, "hash": "442f86ed99e0efa803562a8e17c21199f10f73e0a350f9a1ef410911bfdcc974", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The End of Manual Decoding: Towards Truly End-to-End Language Models\nThe End of Manual Decoding:\nTowards Truly End-to-End Language Models\nZhichao Wang\u2217,1,2 , Dongyang Ma\u2217,1 , Xinting Huang1 , Deng Cai1 , Tian Lan1 , Jiahao Xu1 ,\nHaitao Mi1 , Xiaoying Tang\u2020 ,2 , and Yan Wang\u2217,\u2020,1\n1Tencent AI Lab\n2The Chinese University of Hong Kong, Shenzhen\nzhichaowang@link.cuhk.edu.cn dongyangma@tencent.com\ntangxiaoying@cuhk.edu.cn yanwang.branden@gmail.com\n/githubCode\nModels\nAbstract\nThe \u201cend-to-end\u201d label for LLMs is a misnomer. In practice, they depend on a non-\ndifferentiable decoding process that requires laborious, hand-tuning of hyperparameters\nlike temperature and top-p. This paper introducesAutoDeco, a novel architecture\nthat enables truly \u201cend-to-end\u201d generation by learning to control its own decoding\nstrategy. We augment the standard transformer with lightweight heads that, at each\nstep, dynamically predict context-specific temperature and top-p values alongside the\nnext-token logits. This approach transforms decoding into a parametric, token-level\nprocess, allowing the model to self-regulate its sampling strategy within a single forward\npass.\nThrough extensive experiments on eight benchmarks, we demonstrate thatAutoDeco\nnot only significantly outperforms default decoding strategies but also achieves perfor-\nmance comparable to an oracle-tuned baseline derived from \u201chacking the test set\u201d\u2014a\npractical upper bound for any static method. Crucially, we uncover an emergent ca-\npability for instruction-based decoding control: the model learns to interpret natural\nlanguage commands (e.g., \u201cgenerate with low randomness\u201d) and adjusts its predicted\ntemperature and top-p on a token-by-token basis, opening a new paradigm for steerable\nand interactive LLM decoding.\n1 Introduction\nLLMs have become the de-facto standard in NLP , yet the quality of their generated text hinges on a\nsurprisingly manual and heuristic process: the selection of decoding hyperparameters. Parameters\nlike temperature, top-p, and top-k must be carefully chosen through a task-dependent process of\nmanual sweeps and post-hoc filtering (Shi et al., 2024). This not only incurs significant computational\nand human costs but also profoundly impacts the final output\u2019s creativity, diversity, and factual\ncorrectness, undermining the promise of a truly \u201cend-to-end\u201d system.\nThis reliance on static, hand-tuned parameters creates fundamental bottlenecks. Firstly, the search\nfor an optimal configuration is widely acknowledged as a laborious process because the ideal\nsettings are highly task-dependent; commercial API providers like DeepSeek, for instance, explicitly\nrecommend different temperature settings for distinct application scenarios1. However, this problem,\nruns even deeper: a single static configuration is inherently suboptimal because the ideal level of\nstochasticity varies dramatically within a single generation. For instance, a model might need high\ncreativity to explore initial reasoning paths but high precision to deliver the final answer. This\n\u2217Equal Contribution. The work was done when Zhichao Wang was interning at Tencent AI Lab.\n\u2020Correspondence to: Xiaoying Tang and Yan Wang.\n1https://api-docs.deepseek.com/quick_start/parameter_settings\n1\narXiv:2510.26697v1  [cs.CL]  30 Oct 2025", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}