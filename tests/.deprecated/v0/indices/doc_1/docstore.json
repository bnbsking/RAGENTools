{"docstore/metadata": {"ed0e2bfb-621a-466a-997d-42e7762cd03b": {"doc_hash": "71da7bba69dbd77737dda8b00140fb2c29245355893aaae5db59c3000e5e2dc7"}, "a850d716-502a-493e-bd3e-9812c979c2a2": {"doc_hash": "96e6a903467da6136da6a0618a81edbb672eefaa7d84e3ef42769cd4d40cf058", "ref_doc_id": "ed0e2bfb-621a-466a-997d-42e7762cd03b"}}, "docstore/ref_doc_info": {"ed0e2bfb-621a-466a-997d-42e7762cd03b": {"node_ids": ["a850d716-502a-493e-bd3e-9812c979c2a2"], "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "/app/rags/medical/data/paper.pdf", "file_type": "application/pdf", "file_size": 2468446, "creation_date": "2025-11-02", "last_modified_date": "2025-11-02"}}}, "docstore/data": {"a850d716-502a-493e-bd3e-9812c979c2a2": {"__data__": {"id_": "a850d716-502a-493e-bd3e-9812c979c2a2", "embedding": null, "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "/app/rags/medical/data/paper.pdf", "file_type": "application/pdf", "file_size": 2468446, "creation_date": "2025-11-02", "last_modified_date": "2025-11-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed0e2bfb-621a-466a-997d-42e7762cd03b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "/app/rags/medical/data/paper.pdf", "file_type": "application/pdf", "file_size": 2468446, "creation_date": "2025-11-02", "last_modified_date": "2025-11-02"}, "hash": "71da7bba69dbd77737dda8b00140fb2c29245355893aaae5db59c3000e5e2dc7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The End of Manual Decoding: Towards Truly End-to-End Language Models\nFigure 1: An overview of our proposed end-to-end decoding architecture compared to manual\ndecoding. Our method dynamically predicts temperature and top-p values from the model\u2019s hidden\nstates for each generation step. In contrast, manual decoding (bottom) relies on a single set of static,\npredefined hyperparameters for the entire sequence generation.\non-the-fly control is, by design, impossible for current LLMs to achieve natively. Consequently,\nthe prevailing static decoding paradigm is a solution as inefficient as it is ineffective, forcing a\none-size-fits-all strategy onto a problem that demands dynamic adaptation.\nIn this paper, we proposeAutoDeco, a novel architecture that creates a truly \u201cend-to-end\u201d language\nmodel capable of controlling its own decoding process. As illustrated in Figure 1, we augment\nthe standard transformer with lightweight, dedicated prediction heads. At each decoding step,\ntheseAutoDecoheads leverage the model\u2019s current hidden state to dynamically predict the optimal\nsampling parameters for the next token. This seamlessly integrates hyperparameter selection into\nthe model\u2019s forward pass, creating a self-regulating inference pipeline that adds nearly-zero latency.\nWe validate our approach by integratingAutoDecointo major model families, including Qwen,\nLlama, and GPT, requiring only a brief fine-tuning process of 400 steps. Across eight distinct\nbenchmarks, the results are striking:AutoDeconot only consistently outperforms standard default\ndecoding settings but also matches or surpasses the performance of meticulously expert-guided\ntuning (an oracle-tuned baseline derived from \u201chacking the test set\u201d) hyperparameters. Perhaps\nmost excitingly, we uncovered an emergent capability for instruction-based decoding control. When\nprompted with a meta-instruction like, \u201cPlease ensure that the diversity of your output is low,\u201d the\nmodel immediately responded by lowering its average predicted temperature and top-p values by\n0.11 and 0.06, respectively. This demonstrates thatAutoDecodoes not merely automate a tedious\nprocess; it endows the model with a new, intuitive way to interpret and act on user intent.\nOur contributions are four-fold:(i)We proposeAutoDeco, a novel and lightweight architecture, along\nwith an efficient strategy to train its prediction heads, that makes LLM generation truly \u201cend-to-end\u201d\nby dynamically predicting decoding parameters at each step.(ii)We demonstrate through extensive\nexperiments thatAutoDecoconsistently matches or exceeds the performance of expert-guided tuning,\nstatic hyperparameters across eight benchmarks and multiple model families.(iii)We demonstrate\nfor the first time that an LLM\u2019s decoding can be controlled by natural language. We achieve this by\nobserving a nascent emergent ability and then solidifying it via targeted training, achieving 95%+\nconsistency in steering sampling behavior.(iv)In addition to the models evaluated in this paper, we\nalso releaseAutoDecoheads for Deepseek-V3.1-Terminus, Qwen3-235B-A22B-Thinking-2507, and\nGPT-OSS-120B. However, due to the substantial computational cost of evaluating these large-scale\nmodels, a comprehensive benchmark of these models was not performed.\n2 AudoDeco\nThe foregoing discussion raises two fundamental questions that frame the core inquiry of this work:\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3383, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}