{"docstore/metadata": {"0bbd5843-ff22-4b9a-b8ff-f0cc5c898967": {"doc_hash": "60e6060a87ba9d6c1cb29384c4686f50a9753769b4db167a3634c4522207b2d6"}, "afb7bc1a-3805-40fb-a920-442abf280909": {"doc_hash": "310c13b754d59448439a5ffb99e5736b235de82d206ade1ae940d3685c389bcd", "ref_doc_id": "0bbd5843-ff22-4b9a-b8ff-f0cc5c898967"}}, "docstore/ref_doc_info": {"0bbd5843-ff22-4b9a-b8ff-f0cc5c898967": {"node_ids": ["afb7bc1a-3805-40fb-a920-442abf280909"], "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "/app/rags/medical/data/paper.pdf", "file_type": "application/pdf", "file_size": 2468446, "creation_date": "2025-11-02", "last_modified_date": "2025-11-02"}}}, "docstore/data": {"afb7bc1a-3805-40fb-a920-442abf280909": {"__data__": {"id_": "afb7bc1a-3805-40fb-a920-442abf280909", "embedding": null, "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "/app/rags/medical/data/paper.pdf", "file_type": "application/pdf", "file_size": 2468446, "creation_date": "2025-11-02", "last_modified_date": "2025-11-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0bbd5843-ff22-4b9a-b8ff-f0cc5c898967", "node_type": "4", "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "/app/rags/medical/data/paper.pdf", "file_type": "application/pdf", "file_size": 2468446, "creation_date": "2025-11-02", "last_modified_date": "2025-11-02"}, "hash": "60e6060a87ba9d6c1cb29384c4686f50a9753769b4db167a3634c4522207b2d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The End of Manual Decoding: Towards Truly End-to-End Language Models\n(a) Top-p mask function.\n (b) An example of top-p sampling probability.\nFigure 2: Comparison of the differentiable soft top-p sampling (decay steepness \u03b1= 30) with\nthe standard hard-cutoff method. (a) illustrates the standard hard-cutoff mask, which has a non-\ndifferentiable step, against our proposed smooth and differentiable soft mask. (b) shows the effect of\napplying both masks to an example original probability distribution, where the soft mask method\nproduces a differentiable probability distribution suitable for \u201cend-to-end\u201d training.\nFirst, how can we train theAutoDecoheads without any token-level \u201cground-truth\u201d labels for\nthe optimal temperature and top-p values? Second, how can these predictions be integrated into\ninference without adding computational latency? This section details our solutions to both.\nIn Section 2.1, we will introduce our training strategy and explain how we train both heads in an\nend-to-end manner. Then, in Section 2.2, we will walk through our inference process. TheAutoDeco\nmodifies the model\u2019s final output probabilities internally\u2014a design that adds absolutely no extra\nlatency. The result is a model that can be used almost exactly like a standard one, requiring only a\n\u201c1-line-change\u201d in a user\u2019s code to unlock its dynamic decoding capabilities.\n2.1 Training Strategy\nThe central challenge in trainingAutoDecois the absence of token-level \u201cground-truth\u201d labels for\nsampling parameters. A natural approach would be to optimize the temperature and top-p heads\ndirectly from the final cross-entropy loss of the generated tokens. However, this path is obstructed\nby the standard top-p sampling algorithm. Its \u201chard cutoff\u201d\u2014retaining only the smallest set of\ntokens whose cumulative probability exceeds a threshold\u2014is a non-differentiable operation, which\nsevers the gradient flow from the loss back to the top-p head.\nTo overcome this, we introduce a novel, differentiable \u201csoft\u201d top-p mechanism that is used during\ntraining, enabling a fully \u201cend-to-end\u201d optimization strategy. Traditional top-p sampling methods\nassign a probability of zero to all tokens beyond the top-p threshold, while our approach is different:\nfor tokens that fall outside the top-p threshold, we apply a differentiable weight scaling. The further\na token is from the threshold, the more its probability is scaled down, eventually approaching zero.\nBelow is the training data stream:\n1. Temperature-Scaled Probabilities:First, we scale the predicted logits l to compute the initial\nprobability distributionpusing the predicted temperature \u02c6T.\np=softmax\n\u0012 l\n\u02c6T\n\u0013\n. (1)\n2. Differentiable Mask Generation:After sorting the probabilities p and calculating their cumu-\nlative sum c, we generate a \u201csoft mask\u201d m(sorted). This is done in a single step that combines\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}